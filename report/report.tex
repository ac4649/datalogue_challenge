%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 1.4 (15/5/16)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com) with extensive modifications by
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside,twocolumn]{article}

\usepackage{blindtext} % Package to generate dummy text throughout this template 

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[english]{babel} % Language hyphenation and typographical rules

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\roman{subsection}} % roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{Adrien Cogny $\bullet$ Apr 2018} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{graphicx} % added graphix package for images
\graphicspath{ {images/} }

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting
\title{User Response Classification Challenge} % Article title
\author{%
\textsc{Adrien Cogny} \\% Your name
\normalsize Cornell Tech \\ % Your institution
\normalsize \href{mailto:ac2753@cornell.edu}{ac2753@cornell.edu} % Your email address
%\and % Uncomment if 2 authors are required, duplicate these 4 lines if more
%\textsc{Jane Smith}\thanks{Corresponding author} \\[1ex] % Second author's name
%\normalsize University of Utah \\ % Second author's institution
%\normalsize \href{mailto:jane@smith.com}{jane@smith.com} % Second author's email address
}
\date{\today} % Leave empty to omit a date
\renewcommand{\maketitlehookd}{%
\begin{abstract}
\noindent Abstract Text % Dummy abstract text - replace \blindtext with your abstract text
\end{abstract}
}

%----------------------------------------------------------------------------------------

\begin{document}

% Print the title
\maketitle

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Introduction}

\lettrine[nindent=0em,lines=3]{C}hatbots are used in many applications today: customer support,
flight booking, scheduling meeting, ordering food and many more. The application of a chatbot
 explored in this dataset is for a therapy chatbot. These types of chatbot, while very effective,
 may require human intervention. Determining when a human should intervene can be quite important,
 in this case when a person requires help in dealing with a complex situation, and requires tools
 to identify these situations. 

The data set contains 80 examples of responses entered into a therapy chatbot. Each of these responses
contains an id as well as an identification. The identification is either "flagged" if the response
was flagged for human intervention or "not flagged" if not.

The task at hand was to create an AI agent to classify the user response. 
%------------------------------------------------

\section{Tools}

The following tools and modules were used to complete this task:
\begin{itemize}
\item python 3.
\item pandas
\item dy-net (for the RNN)
\item scikit learn (for the Random Forest)
\item numpy
\item tqdm (for progress bars)
\item csv (For reading the csv embeddings to pandas)
\item re (regular expressions) (for cleaning the data)
\end{itemize}
 

%------------------------------------------------
\section{Preparing Data}
The input data being sentences had to be cleaned up before passing into the models.

The first step was to load the csv file into a pandas dataframe and see what the data looked like.
The data was, as mentioned above, a label as well as a sequence of words (not an array implementation yet).
Due to the inherent nature of natural language processing both the label and sequence of words had to
be converted to something which the machine could understand. That is, the label had to be converted from 
"flagged" or "not flagged" to 1 or 0 respectively and the sequence had to be converted to a series of word
embeddings where each embedding represented a single word.

Natural language contains many words that are very common in sentences. These so-called stop words
 ("their", "he", "she", etc...) can make classifying a sentence very hard as, when combining word
 embeddings, they will take over the representation of the sentence simply by shear number. That is 
 why in the pre-processing of each sentence (before it is given to the model), the stop words were removed
 from the sentence. By removing the stop words, we do not loose much important information and
 are able to classify more easily.

The models were created to do the conversion from label to 1 or 0 and from sentence of words to sequence of embeddings.
The embeddings used were the GloVe embeddings due to their versatility, however another embedding which could have been
used is the word2vec embedding.

\section{Models}

\subsection{RNN}

When looking at sentence classification, one of the first thought was too look at an RNN encoder
that would encode the sentence word by word and the computing a probability of being "flagged"
or "not flagged". The label with the highest probability would then be applied to the sentence input.

\subsubsection{RNN Description}

DESCRIPTION OF RNN AND IMAGE/SKETCH OF THE MODEL CRATED

\subsubsection{Tuning Parameters}

When the model was created, the different parameters were tuned:
\begin{itemize}
  \item{Embedding Dimmension}
  \item{Hidden Dimmension Size}
  \item{Number of Epochs Run}
\end{itemize}

The results for all of these tuning experiments are shown in the results section.


\subsection{Random Forest}

After getting results for the RNN encoder and finding the 

%------------------------------------------------

\section{Results}

\subsection{RNN}

\subsubsection{hidden dimmension}

The hidden dimmension test was done by keeping all parameters of the RNN constant except for the 
hidden dimmension of the RNN. The following figure shows the results for the hidden dimmension
test performed. The test was performed by changing the size of the embedding dimmension from 0 to 9
dimmensions in steps of 1. The training loss, dev set true positive #, dev set true negative #,
dev set false positive # and dev set false negative # were computed for each of the models run 
and a graph was created showing the true positive rate and the false positive rate.

\begin{figure}[h]
\includegraphics[width=6cm]{hidden_dimm_test_table}
\centering
\caption{Shows the table of hidden dimmension tests for the RNN}
\end{figure}

\begin{figure}[h]
\includegraphics[width=6cm]{hidden_dimm_test_graph}
\centering
\caption{Shows the graph of true positive count vs hidden dimmension of RNN}
\end{figure}

This graph shows that the true positive count is always lower than the false negative count.
This means that there are more instances where the model will classify a sentence as "not flagged"
when in fact it should be "flagged" than there are instances where the model correctly classifies
a sentence as "flagged." While this points towards this particular model
(with the hyperparameters described below) not being good, the true positive rate and accuracies are
derived (and much more important) metrics to look at.

While this graphs shows the true positive count as well as the false negative counts,
a more interesting metric which can be derived fromt he true positive count and
the false negative count is the true positive rate (tpr) which shows how much 
of the truth the model captures.

The following figure shows the tpr for the RNN model for different hidden dimmensions.

\begin{figure}[h]
\includegraphics[width=6cm]{hidden_dimm_test_tpr-graph}
\centering
\caption{Shows the graph of true positive rate vs hidden dimmension of RNN}
\end{figure}

From this figure, we can clearly see that the tpr is greatest when the hidden dimmension is 3,
 with the other parameters set to: number of epochs ran = 400, number of layers = 1 and embedding 
 size = 50. This model received an accuracy of 0.591667 and a true positive rate of 0.347826.

While the graph of accuracies shows that when the hidden dimension is of size 0, the accuracy jumps
to ~0.7, this model would not be considered to be a good model as the true positive rate is non existant
because we are not classifying any results as being "flagged", which defeats the whole purpose of the model.


Thus, the hidden dimmension will be set to 3 for the other models.

\subsubsection{GloVe embedding size}

\subsection{Random Forest}

\begin{table}[h]
\caption{Example table}
\centering
\begin{tabular}{llr}
\toprule
\multicolumn{2}{c}{Name} \\
\cmidrule(r){1-2}
First name & Last Name & Grade \\
\midrule
John & Doe & $7.5$ \\
Richard & Miles & $2$ \\
\bottomrule
\end{tabular}
\end{table}

\blindtext % Dummy text

\begin{equation}
\label{eq:emc}
e = mc^2
\end{equation}

\blindtext % Dummy text

%------------------------------------------------

\section{Discussion}

\subsection{Subsection One}

A statement requiring citation \cite{Figueredo:2009dg}.
\blindtext % Dummy text

\subsection{Subsection Two}

\blindtext % Dummy text

%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\begin{thebibliography}{99} % Bibliography - this is intentionally simple in this template

\bibitem[Figueredo and Wolf, 2009]{Figueredo:2009dg}
Figueredo, A.~J. and Wolf, P. S.~A. (2009).
\newblock Assortative pairing and life history strategy - a cross-cultural
  study.
\newblock {\em Human Nature}, 20:317--330.
 
\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{document}
